---
title: "Predicting the brexit vote"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data

- In this exercise, we will work on a classification task of Brexit referendum vote
- The data is originally from British Election Study Online Panel
  - codebook: https://www.britishelectionstudy.com/wp-content/uploads/2020/05/Bes_wave19Documentation_V2.pdf
- The outcome is `LeaveVote` (1: Leave, 0: Remain)

## Libraries

- We will use the following packages

```{r}
library(tidyverse)
library(caret)
library(glmnet)
```

## Load data

We sub-sample the data. Full data takes too much time to estimate for the class... (Feel free to run full sample after the class)

```{r}
set.seed(20200813)
df_brexit <- read_csv("data/data_bes.csv.gz") %>%
  sample_n(3000) # sampling data so
```


## Data preparation

- We will carry out:
  - make `LeaveVote` factor variable
  - test train split
  - preprocess


```{r}
df_brexit <- df_brexit %>%
    mutate(LeaveVote = factor(LeaveVote))
```

### Train-test split

```{r}
set.seed(12345)
train_idx <- createDataPartition(df_brexit$LeaveVote, p = .7, list = F) %>%
  as.vector

df_train <- df_brexit %>% slice(train_idx)
df_test <- df_brexit %>% slice(-train_idx)
```

### Preprocess

```{r}
prep <- preProcess(df_train %>% select(-LeaveVote), method = c("center", "scale"))
prep

df_train_preped <- predict(prep, df_train)
df_test_preped <- predict(prep, df_test)

```

## Model formulas

There are four logistic regression models  in the manuscript (Table 2).

1. Sociodemographics
2. Identity
3. Anti-elite
4. Attitudes

The following line of codes will generate the each model. 

```{r}
fm_socdem <- formula("LeaveVote ~ gender + age + edlevel + hhincome + econPersonalRetro1")
fm_identity <- formula("LeaveVote ~ gender + age + edlevel + hhincome + 
                        EuropeanIdentity + EnglishIdentity + BritishIdentity")
fm_antielite <- formula("LeaveVote ~ gender + age + edlevel + hhincome + 
              PolMistrust + GovDisapproval + PopulismScale + 
              ConVote + LabVote + LibVote + SNPPCVote + UKIP")
fm_attitudes <- formula("LeaveVote ~ gender + age + edlevel + hhincome + euUKNotRich + 
              euNotPreventWar + FreeTradeBad + euParlOverRide1 + euUndermineIdentity1 + 
              lessEUmigrants + effectsEUTrade1 + effectsEUImmigrationLower")
fm_all <- formula("LeaveVote ~ .")


```

You can use these formulas in a way like:

```{r eval = F}
# for model
glm(fm_socdem, data = df_train_preped, family = "binomial")
# for data extraction
model.matrix(fm_socdem, data = df_train_preped) %>% head()

```

## Logistic regression

Run a few models, and evaluate them. Which one has the better predictive performance?

```{r}
print_conf <- function(fm) {mod_log_test <- glm(fm, data = df_train_preped, family = "binomial")

pred_log_train <- mod_log_test %>% predict(newdata = df_train_preped, type = 'response')
pred_log_train <- as.numeric(pred_log_train > .5) %>% factor()

confusionMatrix(pred_log_train, df_train_preped$LeaveVote,
                positive = '1', mode = "prec_recall")


pred_log_test <- mod_log_test %>% predict(newdata = df_test_preped, type = 'response')
pred_log_test <- as.numeric(pred_log_test > .5) %>% factor()

print(confusionMatrix(pred_log_test, df_test_preped$LeaveVote,
                positive = '1', mode = "prec_recall"))
}

print_conf(fm_socdem)
print_conf(fm_identity)
print_conf(fm_antielite)
print_conf(fm_attitudes)
print_conf(fm_all)
```

###jei nori istraukti kazkoki rodmeni is confussion matrix
conf_m <- confusionMatrix(pred_log_test, df_test_preped$LeaveVote,
                positive = '1', mode = "prec_recall")
                
                conf_m$byClass['F1']
## Linear SVM

- Train a linear SVM model, check the predictive performance. How does it compare to the logistic regression?

```{r}

```


## Polynomial SVM and Radial SVM

- Train non-linear SVM. How is the performance? Any improvement?

```{r cache=T}


```


## (Optional) Logistic regression with LASSO

- `glmnet` can run a Logistic model with L1 penalty (LASSO). 
- Try a "full" model combining all inputs.
  - Which inputs survived?

```{r}

```


