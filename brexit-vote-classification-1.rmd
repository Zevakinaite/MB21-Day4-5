---
title: "Predicting the brexit vote"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data

- In this exercise, we will work on a classification task of Brexit referendum vote
- The data is originally from British Election Study Online Panel
  - codebook: https://www.britishelectionstudy.com/wp-content/uploads/2020/05/Bes_wave19Documentation_V2.pdf
- The outcome is `LeaveVote` (1: Leave, 0: Remain)

## Libraries

- We will use the following packages

```{r}
library(tidyverse)
library(caret)
library(glmnet)
```

## Load data

We sub-sample the data. Full data takes too much time to estimate for the class... (Feel free to run full sample after the class)

```{r}
set.seed(20200813)
df_brexit <- read_csv("data/data_bes.csv.gz") %>%
  sample_n(3000) # sampling data so
```


## Data preparation

- We will carry out:
  - make `LeaveVote` factor variable
  - test train split
  - preprocess


```{r}
df_brexit <- df_brexit %>%
    mutate(LeaveVote = factor(LeaveVote))
```

### Train-test split

```{r}
set.seed(12345)
train_idx <- createDataPartition(df_brexit$LeaveVote, p = .7, list = F) %>%
  as.vector

df_train <- df_brexit %>% slice(train_idx)
df_test <- df_brexit %>% slice(-train_idx)
```

### Preprocess

```{r}
prep <- preProcess(df_train %>% select(-LeaveVote), method = c("center", "scale"))
prep

df_train_preped <- predict(prep, df_train)
df_test_preped <- predict(prep, df_test)

```

## Model formulas

There are four logistic regression models  in the manuscript (Table 2).

1. Sociodemographics
2. Identity
3. Anti-elite
4. Attitudes

The following line of codes will generate the each model. 

```{r}
fm_socdem <- formula("LeaveVote ~ gender + age + edlevel + hhincome + econPersonalRetro1")
fm_identity <- formula("LeaveVote ~ gender + age + edlevel + hhincome + 
                        EuropeanIdentity + EnglishIdentity + BritishIdentity")
fm_antielite <- formula("LeaveVote ~ gender + age + edlevel + hhincome + 
              PolMistrust + GovDisapproval + PopulismScale + 
              ConVote + LabVote + LibVote + SNPPCVote + UKIP")
fm_attitudes <- formula("LeaveVote ~ gender + age + edlevel + hhincome + euUKNotRich + 
              euNotPreventWar + FreeTradeBad + euParlOverRide1 + euUndermineIdentity1 + 
              lessEUmigrants + effectsEUTrade1 + effectsEUImmigrationLower")
fm_all <- formula("LeaveVote ~ .")


```

You can use these formulas in a way like:

```{r eval = F}
# for model
glm(fm_socdem, data = df_train_preped, family = "binomial")
# for data extraction
model.matrix(fm_socdem, data = df_train_preped) %>% head()

```

## Logistic regression

Run a few models, and evaluate them. Which one has the better predictive performance?

```{r}
print_conf <- function(fm) {mod_log_test <- glm(fm, data = df_train_preped, family = "binomial")

pred_log_train <- mod_log_test %>% predict(newdata = df_train_preped, type = 'response')
pred_log_train <- as.numeric(pred_log_train > .5) %>% factor()

confusionMatrix(pred_log_train, df_train_preped$LeaveVote,
                positive = '1', mode = "prec_recall")


pred_log_test <- mod_log_test %>% predict(newdata = df_test_preped, type = 'response')
pred_log_test <- as.numeric(pred_log_test > .5) %>% factor()

print(confusionMatrix(pred_log_test, df_test_preped$LeaveVote,
                positive = '1', mode = "prec_recall"))
}

print_conf(fm_socdem)
print_conf(fm_identity)
print_conf(fm_antielite)
print_conf(fm_attitudes)
print_conf(fm_all)
```

###jei nori istraukti kazkoki rodmeni is confussion matrix
conf_m <- confusionMatrix(pred_log_test, df_test_preped$LeaveVote,
                positive = '1', mode = "prec_recall")
                
                conf_m$byClass['F1']
## Linear SVM

- Train a linear SVM model, check the predictive performance. How does it compare to the logistic regression?

```{r}

```


## Polynomial SVM and Radial SVM

- Train non-linear SVM. How is the performance? Any improvement?

```{r cache=T}


```


## (Optional) Logistic regression with LASSO

- `glmnet` can run a Logistic model with L1 penalty (LASSO). 
- Try a "full" model combining all inputs.
  - Which inputs survived?

```{r}

```

##############
# FROM CLASS #
##############


## Data preparation

- We will carry out:
  - make `LeaveVote` factor variable
  - test train split
  - preprocess


```{r}
df_brexit <- df_brexit %>%
    mutate(LeaveVote = factor(LeaveVote))
```

### Train-test split

```{r}
train_idx <- createDataPartition(df_brexit$LeaveVote, p = .7, list = F) 
df_train <- df_brexit %>% slice(train_idx)
df_test <- df_brexit %>% slice(-train_idx)
```

### Preprocess

```{r}
prep <- preProcess(df_train %>% select(-LeaveVote), method = c("center", "scale"))
prep
df_train_preped <- predict(prep, df_train)
df_test_preped <- predict(prep, df_test)
```

## Model formulas

There are four logistic regression models  in the manuscript (Table 2).

1. Sociodemographics
2. Identity
3. Anti-elite
4. Attitudes

The following line of codes will generate the each model. 

```{r}
fm_socdem <- formula("LeaveVote ~ gender + age + edlevel + hhincome + econPersonalRetro1")
fm_identity <- formula("LeaveVote ~ gender + age + edlevel + hhincome + 
                        EuropeanIdentity + EnglishIdentity + BritishIdentity")
fm_antielite <- formula("LeaveVote ~ gender + age + edlevel + hhincome + 
              PolMistrust + GovDisapproval + PopulismScale + 
              ConVote + LabVote + LibVote + SNPPCVote + UKIP")
fm_attitudes <- formula("LeaveVote ~ gender + age + edlevel + hhincome + euUKNotRich + 
              euNotPreventWar + FreeTradeBad + euParlOverRide1 + euUndermineIdentity1 + 
              lessEUmigrants + effectsEUTrade1 + effectsEUImmigrationLower")
fm_all <- formula("LeaveVote ~ .")
```

You can use these formulas in a way like:

```{r eval = F}
# for model
socdem<- glm(fm_socdem, data = df_train_preped, family = "binomial")
# for data extraction
model.matrix(fm_socdem, data = df_train_preped) %>% head()
```

## Logistic regression

Run a few models, and evaluate them. Which one has the better predictive performance?

```{r}
socdem<- glm(fm_socdem, data = df_train_preped, family = "binomial")
identity<- glm(fm_identity, data = df_train_preped, family = "binomial")
antielite<- glm(fm_antielite, data = df_train_preped, family = "binomial")
attitudes<- glm(fm_attitudes, data = df_train_preped, family = "binomial")
socdem$fitted.values <- if_else(socdem$fitted.values>0.5,1,0)
socdem_train_metrics <- confusionMatrix(as.factor(socdem$fitted.values),df_train_preped$LeaveVote, positive = "1",mode = "prec_recall")
predict_socdem<- predict(socdem, df_test_preped, type = "response")
predict_socdem <- if_else(predict_socdem>.5,1,0)
socdem_test_metrics <- confusionMatrix(as.factor(predict_socdem),df_test_preped$LeaveVote, positive = "1",mode = "prec_recall")
identity$fitted.values <- if_else(identity$fitted.values>0.5,1,0)
identity_train_metrics <- confusionMatrix(as.factor(identity$fitted.values),df_train_preped$LeaveVote, positive = "1",mode = "prec_recall")
predict_identity<- predict(identity, df_test_preped, type = "response")
predict_identity <- if_else(predict_identity>.5,1,0)
identity_test_metrics <- confusionMatrix(as.factor(predict_identity),df_test_preped$LeaveVote, positive = "1",mode = "prec_recall")
antielite$fitted.values <- if_else(antielite$fitted.values>0.5,1,0)
antielite_train_metrics <- confusionMatrix(as.factor(antielite$fitted.values),df_train_preped$LeaveVote, positive = "1",mode = "prec_recall")
predict_antielite<- predict(antielite, df_test_preped, type = "response")
predict_antielite <- if_else(predict_antielite>.5,1,0)
antielite_test_metrics <- confusionMatrix(as.factor(predict_antielite),df_test_preped$LeaveVote, positive = "1",mode = "prec_recall")
attitudes$fitted.values <- if_else(attitudes$fitted.values>0.5,1,0)
attitudes_train_metrics_logistic <- confusionMatrix(as.factor(attitudes$fitted.values),df_train_preped$LeaveVote, positive = "1",mode = "prec_recall")
predict_attitudes_test_logistic<- predict(attitudes, df_test_preped, type = "response")
predict_attitudes_test_logistic <- if_else(predict_attitudes_test_logistic>0.5,1,0)
attitudes_test_metrics_logistic <- confusionMatrix(as.factor(predict_attitudes_test_logistic),df_test_preped$LeaveVote, positive = "1",mode = "prec_recall")
###the attitudes model seems to be the best and then the identity one the second best
```

## Linear SVM

- Train a linear SVM model, check the predictive performance. How does it compare to the logistic regression?

```{r}
control <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
svm_attitudes <- train(fm_attitudes, data = df_train_preped, method = "svmLinear", tr = control)
predict_attitudes_train_svm <- predict(svm_attitudes)
svm_attitudes_train_metrics <- confusionMatrix(predict_attitudes_train_svm, df_train_preped$LeaveVote, positive = "1", mode = "prec_recall")
predict_attitudes_test_svm <- predict(svm_attitudes, df_test_preped)
svm_attitudes_test_metrics <- confusionMatrix(predict_attitudes_test_svm, df_test_preped$LeaveVote, positive = "1", mode = "prec_recall")
###both the logistic and linear svm attitudes models are very very good in terms of all the metrics
###we can see the two roc auc curves for the test set 
library(pROC)
roc(df_test_preped$LeaveVote, as.ordered(predict_attitudes_test_svm), plot = TRUE, legacy.axes = TRUE, percent = TRUE, xlab="False Positive", ylab = "True Positive",col = "green", lwd = 4,print.auc = TRUE, print.auc.x = 40)
plot.roc(df_test_preped$LeaveVote, as.ordered(predict_attitudes_test_logistic), percent = TRUE, col = "blue", print.auc = TRUE, add = TRUE, print.auc.y = 30)
plot.roc(df_test_preped$LeaveVote, as.ordered(predict_attitudes_test_poly), percent = TRUE, col = "red", print.auc = TRUE, add = TRUE, print.auc.y = 20)
legend("bottomright", legend = c("Svm", "Logistic Regression","Svm Polynomial Kernel"), col = c("green", "blue","red"), lwd = 4)
```


## Polynomial SVM and Radial SVM

- Train non-linear SVM. How is the performance? Any improvement?

```{r cache=T}
poly_svm_attitudes <- train(fm_attitudes, data = df_train_preped, method = "svmPoly", tr = control)
predict_attitudes_train_poly <- predict(poly_svm_attitudes)
poly_svm_attitudes_train_metrics <- confusionMatrix(predict_attitudes_train_poly, df_train_preped$LeaveVote, positive = "1", mode = "prec_recall")
predict_attitudes_test_poly <- predict(poly_svm_attitudes, df_test_preped)
poly_svm_attitudes_test_metrics <- confusionMatrix(predict_attitudes_test_poly, df_test_preped$LeaveVote, positive = "1", mode = "prec_recall")
```


## (Optional) Logistic regression with LASSO

- `glmnet` can run a Logistic model with L1 penalty (LASSO). 
- Try a "full" model combining all inputs.
  - Which inputs survived?

```{r}
df_train_preped_x <- df_train_preped %>% select(-LeaveVote) %>% as.matrix()
df_train_preped_y <- df_train_preped$LeaveVote
df_test_preped_x <- df_test_preped %>% select(-LeaveVote) %>% as.matrix()
df_test_preped_y <- df_test_preped$LeaveVote
lasso_log_reg <- cv.glmnet(df_train_preped_x,df_train_preped_y, alpha = 1, type.measure = "mse", family = "binomial" )
plot(lasso_log_reg)
coef(lasso_log_reg)
###seems that lasso keeps all the attitudes variables
```
